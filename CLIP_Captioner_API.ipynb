{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **CLIP Captioner API**\n",
        "\n",
        "A API endpoint for using OpenAI Clip to caption images.\n",
        "\n",
        "This is a API meant to be used with tools for automating captioning images. You can use this colab notebook if you don't have a GPU.\n",
        "\n",
        "*Author:* [_CypherpunkSamurai_](https://github.com/CypherpunkSamurai)\n",
        "\n",
        "If you find any bugs feel free to contact me ðŸ˜Š\n",
        "\n",
        "## Credits\n",
        "- OpenAI CLIP\n",
        "- pharmapsychotic (for the CLIP2 Colab)"
      ],
      "metadata": {
        "id": "bi4vbMeomWwf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**Shameless Self-promotion:**\n",
        "Checkout my QT5 based captioning tool [Captioner](https://github.com/CypherpunkSamurai/captioner) ðŸ˜›"
      ],
      "metadata": {
        "id": "4HiDPrg_JmB-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "ozO5Q8o7mm3s"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "kxmeRIQlmTIj",
        "outputId": "79344fd9-840c-4cb6-9103-830d2b212184"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Feb 19 19:07:26 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 510.47.03    Driver Version: 510.47.03    CUDA Version: 11.6     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   42C    P8     9W /  70W |      3MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "#@markdown # Check Colab GPU\n",
        "#@markdown Check if GPU is available. If not switch runtime.\n",
        "\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown # Install requirements\n",
        "#@markdown Install the required modules to run CLIP\n",
        "!pip -q install gradio open_clip_torch clip-interrogator fastapi nest-asyncio python-multipart uvicorn pyngrok"
      ],
      "metadata": {
        "cellView": "form",
        "id": "p-0JIdJ0nQRx"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## Config\n",
        "#@markdown Choose a Model.\n",
        "#@markdown - ViT-L-14 works good for 1.0 to 1.4\n",
        "#@markdown - ViT-H-14 works good for 1.5 to 2.*.\n",
        "clip_model_name = 'ViT-H-14/laion2b_s32b_b79k' #@param [\"ViT-L-14/openai\", \"ViT-H-14/laion2b_s32b_b79k\"]"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qxj6sHEmn0YA"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ## Setup CLIP\n",
        "import gradio as gr\n",
        "from clip_interrogator import Config, Interrogator\n",
        "\n",
        "# Interrogator Conf\n",
        "config = Config(\n",
        "  clip_model_name=clip_model_name,\n",
        "  clip_model_path='cache',\n",
        "  device='cuda:0',\n",
        "  blip_num_beams=64,\n",
        "  blip_offload=False\n",
        ")\n",
        "\n",
        "#@markdown\n",
        "# Custom Checkpoints for CLIP\n",
        "# Comment to use official CLIP Model\n",
        "# !apt-get install -qy aria2 > /dev/null\n",
        "# !mkdir -p \"cache\"\n",
        "# !aria2c -x 16 'https://huggingface.co/pharma/ci-preprocess/resolve/main/ViT-H-14_laion2b_s32b_b79k_artists.pkl' \\\n",
        "#     'https://huggingface.co/pharma/ci-preprocess/resolve/main/ViT-H-14_laion2b_s32b_b79k_flavors.pkl' \\\n",
        "#     'https://huggingface.co/pharma/ci-preprocess/resolve/main/ViT-H-14_laion2b_s32b_b79k_mediums.pkl' \\\n",
        "#     'https://huggingface.co/pharma/ci-preprocess/resolve/main/ViT-H-14_laion2b_s32b_b79k_movements.pkl' \\\n",
        "#     'https://huggingface.co/pharma/ci-preprocess/resolve/main/ViT-H-14_laion2b_s32b_b79k_trendings.pkl' \\\n",
        "#     -d \"cache\"\n",
        "\n",
        "# New Interrogator\n",
        "ci = Interrogator(config)\n",
        "\n",
        "\n",
        "def image_analysis(image):\n",
        "    image = image.convert('RGB')\n",
        "    image_features = ci.image_to_features(image)\n",
        "\n",
        "    top_mediums = ci.mediums.rank(image_features, 5)\n",
        "    top_artists = ci.artists.rank(image_features, 5)\n",
        "    top_movements = ci.movements.rank(image_features, 5)\n",
        "    top_trendings = ci.trendings.rank(image_features, 5)\n",
        "    top_flavors = ci.flavors.rank(image_features, 5)\n",
        "\n",
        "    medium_ranks = {medium: sim for medium, sim in zip(top_mediums, ci.similarities(image_features, top_mediums))}\n",
        "    artist_ranks = {artist: sim for artist, sim in zip(top_artists, ci.similarities(image_features, top_artists))}\n",
        "    movement_ranks = {movement: sim for movement, sim in zip(top_movements, ci.similarities(image_features, top_movements))}\n",
        "    trending_ranks = {trending: sim for trending, sim in zip(top_trendings, ci.similarities(image_features, top_trendings))}\n",
        "    flavor_ranks = {flavor: sim for flavor, sim in zip(top_flavors, ci.similarities(image_features, top_flavors))}\n",
        "    \n",
        "    return medium_ranks, artist_ranks, movement_ranks, trending_ranks, flavor_ranks\n",
        "\n",
        "\n",
        "def image_to_prompt(image, mode):\n",
        "    \"\"\"\n",
        "      Convert Image to prompt\n",
        "    \"\"\"\n",
        "    ci.config.chunk_size = 2048 if ci.config.clip_model_name == \"ViT-L-14/openai\" else 1024\n",
        "    ci.config.flavor_intermediate_count = 2048 if ci.config.clip_model_name == \"ViT-L-14/openai\" else 1024\n",
        "    image = image.convert('RGB')\n",
        "    if mode == 'best':\n",
        "        return ci.interrogate(image)\n",
        "    elif mode == 'classic':\n",
        "        return ci.interrogate_classic(image)\n",
        "    elif mode == 'fast':\n",
        "        return ci.interrogate_fast(image)\n",
        "    elif mode == 'negative':\n",
        "        return ci.interrogate_negative(image)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "cellView": "form",
        "id": "JvQ715ImoSDv",
        "outputId": "3f94a7b8-bef7-4cb1-9468-306de1c4e771"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading BLIP model...\n",
            "load checkpoint from https://storage.googleapis.com/sfr-vision-language-research/BLIP/models/model_large_caption.pth\n",
            "Loading CLIP model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "ViT-H-14_laion2b_s32b_b79k_artists.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 21.6M/21.6M [00:00<00:00, 326MB/s]\n",
            "ViT-H-14_laion2b_s32b_b79k_flavors.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 207M/207M [00:00<00:00, 268MB/s]\n",
            "ViT-H-14_laion2b_s32b_b79k_mediums.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 195k/195k [00:00<00:00, 27.9MB/s]\n",
            "ViT-H-14_laion2b_s32b_b79k_movements.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 410k/410k [00:00<00:00, 23.7MB/s]\n",
            "ViT-H-14_laion2b_s32b_b79k_negative.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 84.2k/84.2k [00:00<00:00, 15.7MB/s]\n",
            "ViT-H-14_laion2b_s32b_b79k_trendings.safetensors: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 148k/148k [00:00<00:00, 26.7MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loaded CLIP model and data in 46.65 seconds.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run the API\n",
        "Run the API and provide a url"
      ],
      "metadata": {
        "id": "cDYp3quzowFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@markdown ### FastAPI Code\n",
        "#@markdown HTTP Server Code for captioning images\n",
        "\n",
        "#@markdown __Available API Endpoints:__\n",
        "#@markdown - `/` - Shows Status\n",
        "#@markdown - `/help` - Shows Help\n",
        "#@markdown - `/caption` - Caption an image\n",
        "#@markdown  - `file` is the image\n",
        "#@markdown  - `prompt_mode` is prompt_mode (best, fast etc. default is `best`)\n",
        "#@markdown - `docs` - Swagger UI\n",
        "\n",
        "# Log\n",
        "import logging\n",
        "import traceback\n",
        "# logging config\n",
        "logging.basicConfig(\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    level=logging.INFO\n",
        ")\n",
        "clip_logger = logging.getLogger('CLIP')\n",
        "\n",
        "# FastAPI\n",
        "from typing import Optional # Optional Feild\n",
        "from fastapi import FastAPI\n",
        "from fastapi import File, UploadFile, Form\n",
        "# Image API\n",
        "from PIL import Image\n",
        "from IPython.display import display\n",
        "from io import BytesIO\n",
        "\n",
        "\n",
        "# new api\n",
        "app = FastAPI()\n",
        "\n",
        "\n",
        "@app.get(\"/\")\n",
        "async def root():\n",
        "  return {\"status\": \"running\", \"response\": {\"clip_model\": clip_model_name} }\n",
        "\n",
        "@app.get(\"/help\")\n",
        "async def root():\n",
        "  return {\"status\": \"ok\", \"response\": \"please check /docs for documentation.\"}\n",
        "\n",
        "@app.post(\"/caption\")\n",
        "async def caption_img(file: UploadFile = File(...), prompt_mode : Optional[str] = Form(\"best\")):\n",
        "    try:\n",
        "        # read \"file\" and \"prompt_mode\" from multipart request\n",
        "        contents = await file.read()\n",
        "        \n",
        "        # read PIL image\n",
        "        img = Image.open(BytesIO(contents))\n",
        "        \n",
        "        # caption\n",
        "        clip_logger.info(f\"captioning image in prompt mode: {prompt_mode}\")\n",
        "        caption = image_to_prompt(img, prompt_mode)\n",
        "        \n",
        "        # return\n",
        "        return {\"status\": \"ok\", \"caption\": caption, \"prompt_mode\": prompt_mode}\n",
        "\n",
        "    except Exception:\n",
        "        return {\"status\": \"error\", \"response\": traceback.format_exc()}\n",
        "    finally:\n",
        "        file.close()"
      ],
      "metadata": {
        "cellView": "form",
        "id": "qEe5lOizo2cY"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "\n",
        "#@markdown ### Enable Ngrok (disable for local machine)\n",
        "use_ngrok = True #@param {type: \"boolean\"}\n",
        "\n",
        "#@markdown Enter `NGROK_TOKEN` to get a public API url\n",
        "if use_ngrok:\n",
        "  NGROK_TOKEN = \"YOUR TOKEN HERE\" #@param {\"type\": \"string\"}\n",
        "  ngrok.set_auth_token(NGROK_TOKEN)\n",
        "\n",
        "  # stop running\n",
        "  ngrok_process = ngrok.get_ngrok_process()\n",
        "  ngrok_process.stop_monitor_thread()\n",
        "  ngrok.kill()\n",
        "\n",
        "  ngrok_tunnel = ngrok.connect(9000)\n",
        "  print(\"Public URL:\", ngrok_tunnel.public_url)\n",
        "  print(\"Note: add `ngrok-skip-browser-warning: True` header to your request to bypass ngrok verification\\n\")\n",
        "\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, port=9000)"
      ],
      "metadata": {
        "cellView": "form",
        "id": "QUo389Qx07iV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "r_oCwlKKHF7I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Thanks for Colaborating! ðŸ˜Š\n",
        "If you find any bugs please let me know on [Github](https://github.com/CypherpunkSamurai)\n",
        "\n",
        "Join the SAIL Discord for more notebooks like this... "
      ],
      "metadata": {
        "id": "AsyGy7Y-HHBO"
      }
    }
  ]
}